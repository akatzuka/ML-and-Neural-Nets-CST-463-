{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final: Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This practical contains 6 problems, labeled 'Problem 1', 'Problem 2', etc.  Please read the instructions carefully.  I estimated 10 minutes work per problem.   Only edit cells that you are directed to edit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission: Please submit your .ipynb file only.  The file you submit must have output that demonstrates your answers.  I recommend restarting and then running all cells before you submit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading: Each problem is worth a maximum of 10 points.  Any points above 50 will be treated as extra credit points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.  Machine learning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section we'll use the \"census summary\" data set, which contains census information plus a label showing whether the income associated with the individual is > $50,000/year.  (The data is from 1994.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD imports to this cell as needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"https://raw.githubusercontent.com/grbruns/cst495/master/1994-census-summary.csv\"\n",
    "dat = pd.read_csv(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some columns\n",
    "dat.drop(['usid', 'fnlwgt', 'education', 'native_country'], axis=1, inplace=True)\n",
    "\n",
    "# remove rows containing NaN\n",
    "dat.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tabulate the number of values in each category of the \"occupation\" column, and plot\n",
    "# as a barplot\n",
    "sns.countplot(y=\"occupation\", orient=\"v\", data=dat)\n",
    "plt.title('Occupation counts');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final preprocessing.   Steps here include creation of dummy variables for categorical variables, creation of new target variable, creation of numeric numpy matrix, and scaling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.get_dummies(dat, columns=['workclass', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'label'], drop_first=True)\n",
    "\n",
    "# derive numpy array y from label column, then remove the column\n",
    "y = dat['label_>50K'].values\n",
    "dat.drop(['label_>50K'], axis=1, inplace=True)\n",
    "\n",
    "# create a numpy matrix X from dat1\n",
    "X = dat.values.astype(float)  # converting to float avoids StandardScalar warnings\n",
    "\n",
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new matrix X_reduced, which will have only 5 features, and then create a training and test set from X_reduced and y.  You can get the 5 features using feature selection, or by using dimensionality reduction.  You can choose your train/test ratio.\n",
    "\n",
    "I will be looking at the accuracy of your classification result, so think about how to create/select your 5 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 5)\n",
    "X_reduced = pca.fit_transform(X_scaled)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced,\n",
    "y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your job here is to create a classifier, train it, and then test your accuracy with your test set.  You can choose your predictor, but only use NumPy, Pandas, and Scikit-Learn, and use only a type of predictor we have seen in class.\n",
    "\n",
    "You are also required to use grid search to find good hyperparameter values for your classifier.  Because you are working under time contraints, it will be enough to check 6 or 8 combinations of hyperparameters.\n",
    "\n",
    "Print your accuracy on the test set.  You should be able to achieve 83% accuracy or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier 0.7861979166666667\n",
      "RandomForestClassifier 0.8244791666666667\n",
      "SVC 0.8305989583333333\n",
      "GradientBoostingClassifier 0.8235677083333334\n",
      "VotingClassifier 0.8287760416666666\n",
      "GridSearchCV 0.8313802083333334\n"
     ]
    }
   ],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators = 60, n_jobs = -1)\n",
    "rnd_clf_fitted = rnd_clf.fit(X_train, y_train)\n",
    "RFC_score = rnd_clf.score(X_test, y_test)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_predict = clf.predict(X_test)\n",
    "\n",
    "svm_clf = SVC(gamma=\"scale\")\n",
    "g_clf = GradientBoostingClassifier(n_estimators = 5, learning_rate = 1.0)\n",
    "voting_clf = VotingClassifier(\n",
    "estimators=[('clf', clf), ('rf', rnd_clf), ('svc', svm_clf), ('gboost', g_clf)],\n",
    "voting='hard')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "for clf1 in (clf, rnd_clf, svm_clf, g_clf, voting_clf):\n",
    "    clf1.fit(X_train, y_train)\n",
    "    y_pred = clf1.predict(X_test)\n",
    "    print(clf1.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "    \n",
    "parameters = {'C': [10], 'gamma': [0.1], 'kernel': ['rbf']}\n",
    "clf2 = GridSearchCV(svm_clf, parameters, cv=6)\n",
    "clf2.fit(X_train, y_train)\n",
    "y_pred = clf2.predict(X_test)\n",
    "print(clf2.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the markdown cell below, please write two or three sentences on why you chose the approach that you did in Problem 2a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For feature selection, I used PCA to reduce the number of features to 5, as this is simple and tends to be the best method of selecting features.\n",
    "\n",
    "For estimators, I tried using a voting classifier which used a random forest, an SVM, and and decision trees as its inputs. This resulted in accuracy that was very close to 83% across multiple attempts, but could not have it break that 83% barrier. Since I was close to the 83% accuracy mark, I tried to use grid search tuning the hyperparameters of the Support Vector Machine, and this improved my accuracy above the 83% mark.\n",
    "\n",
    "Since I got a result very close to 83% accuracy, I then tried tuning the hyperparameters of gradient boosting.  The problem is that grid search took a long time, so I wasn't able to try many different parameter values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.  Deep Learning with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST classification with a CNN, shared header code.  This must be run before working on any of the CNN cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# adjust logging verbosity level\n",
    "old_verbosity = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# load data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "# reset verbosity level\n",
    "tf.logging.set_verbosity(old_verbosity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST classification with a CNN, base code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is a slightly modified version of Geron's MNIST classifier.  It uses two convolutional layers, a pooling layer, and two dense layers.  You should run the code to make sure it works for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# graph building phase\n",
    "#\n",
    "\n",
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "n_inputs = height * width\n",
    "\n",
    "conv1_fmaps = 32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "conv2_fmaps = 64\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "pool3_fmaps = conv2_fmaps\n",
    "\n",
    "n_fc1 = 64\n",
    "n_outputs = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 7 * 7])\n",
    "\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "#\n",
    "# graph execution phase\n",
    "#\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 10\n",
    "test_batch_size = 20\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        # train\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_train, y_train = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_train, y: y_train})\n",
    "        # loss and training accuracy\n",
    "        loss_val, acc_train = sess.run([loss, accuracy], feed_dict={X: X_train, y: y_train})\n",
    "        # test accuracy\n",
    "        X_test, y_test = mnist.test.next_batch(test_batch_size)\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        \n",
    "        print(epoch, \"loss:\", loss_val, \", train accuracy:\", acc_train, \", test accuracy:\", acc_test)\n",
    "        # save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is a copy of the base code.  Your job is to remove the second convolutional layer, and then make the smallest changes needed to get the CNN working again.  Do not add any new layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# graph building phase\n",
    "#\n",
    "\n",
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "n_inputs = height * width\n",
    "\n",
    "conv1_fmaps = 64\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 2\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "#conv2_fmaps = 64\n",
    "#conv2_ksize = 3\n",
    "#conv2_stride = 2\n",
    "#conv2_pad = \"SAME\"\n",
    "\n",
    "#pool3_fmaps = conv2_fmaps\n",
    "pool3_fmaps = conv1_fmaps\n",
    "\n",
    "n_fc1 = 64\n",
    "n_outputs = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "#conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "#                         strides=conv2_stride, padding=conv2_pad,\n",
    "#                         activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    pool3 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 7 * 7])\n",
    "\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "#\n",
    "# graph execution phase\n",
    "#\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 10\n",
    "test_batch_size = 20\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        # train\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_train, y_train = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_train, y: y_train})\n",
    "        # loss and training accuracy\n",
    "        loss_val, acc_train = sess.run([loss, accuracy], feed_dict={X: X_train, y: y_train})\n",
    "        # test accuracy\n",
    "        X_test, y_test = mnist.test.next_batch(test_batch_size)\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        \n",
    "        print(epoch, \"loss:\", loss_val, \", train accuracy:\", acc_train, \", test accuracy:\", acc_test)\n",
    "        # save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is like the base code, except that two new nodes have been introduced: 'probs' and 'y_hot'.  Your job is to change the right hand side of the line 'xentropy = ...', and to replace the TensorFlow function on the right with a different TensorFlow function.  The arguments provided to the new function should include either 'prob' or 'y_hot' or both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# graph building phase\n",
    "#\n",
    "\n",
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "n_inputs = height * width\n",
    "\n",
    "conv1_fmaps = 32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "conv2_fmaps = 64\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "pool3_fmaps = conv2_fmaps\n",
    "\n",
    "n_fc1 = 64\n",
    "n_outputs = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "    y_hot = tf.one_hot(y, n_outputs)\n",
    "\n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 7 * 7])\n",
    "\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
    "    probs = tf.nn.softmax(logits, name=\"probs\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_hot) \n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "#\n",
    "# graph execution phase\n",
    "#\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "test_batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        # train\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_train, y_train = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_train, y: y_train})\n",
    "        # loss and training accuracy\n",
    "        loss_val, acc_train = sess.run([loss, accuracy], feed_dict={X: X_train, y: y_train})\n",
    "        # test accuracy\n",
    "        X_test, y_test = mnist.test.next_batch(test_batch_size)\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        \n",
    "        print(epoch, \"loss:\", loss_val, \", train accuracy:\", acc_train, \", test accuracy:\", acc_test)\n",
    "        # save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-level text modeling with an RNN, shared header code.  This code must be run before working on any of the RNN cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you don't have the Alice.txt file handy, you can get from our class GoogleDrive: https://drive.google.com/open?id=1phSka_OrfrKDajE-0Vyk_AyT4KFb-YdC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Glenn/Google Drive/CSUMB/Spring18/DS/homework/week6/alice.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-78c35d6a2098>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# replace the file path here with your own\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/Glenn/Google Drive/CSUMB/Spring18/DS/homework/week6/alice.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# create mapping from characters to numbers and back\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Glenn/Google Drive/CSUMB/Spring18/DS/homework/week6/alice.txt'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# replace the file path here with your own\n",
    "data = (open(\"C:/Users/Glenn/Google Drive/CSUMB/Spring18/DS/homework/week6/alice.txt\").read())\n",
    "\n",
    "# create mapping from characters to numbers and back\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }   # dict comprehension\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-level text modeling with an RNN, base code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is a simple TensorFlow version of Karpathy's character-level text model.  Make sure you can run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training sequences, and corresponding labels\n",
    "X = []\n",
    "y = []\n",
    "seq_length = 50\n",
    "for i in range(0, len(data)-seq_length-1, 1):\n",
    "    X.append([char_to_ix[ch] for ch in data[i:i+seq_length]])\n",
    "    y.append([char_to_ix[ch] for ch in data[i+1:i+seq_length+1]])\n",
    "\n",
    "# reshape the data; in X_modified, each row is an encoded \n",
    "# sequence of characters\n",
    "X_modified = np.reshape(X, (len(X), seq_length))\n",
    "y_modified = np.reshape(y, (len(y), seq_length))\n",
    "\n",
    "#\n",
    "# graph construction\n",
    "#\n",
    "\n",
    "n_steps = seq_length\n",
    "n_inputs = vocab_size\n",
    "n_outputs = vocab_size\n",
    "n_neurons = 100\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, n_steps])\n",
    "X_hot = tf.one_hot(X, n_inputs)\n",
    "y = tf.placeholder(tf.int32, [None, n_steps])\n",
    "\n",
    "# alternative: GPU-enabled GRU cell\n",
    "#        tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell(num_units=n_neurons),\n",
    "\n",
    "basic_cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "              tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons),\n",
    "              output_size=n_outputs)\n",
    "logits, states = tf.nn.dynamic_rnn(basic_cell, X_hot, dtype=tf.float32)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "# Get the last character of the first batch, then convert it\n",
    "# to a probability vector.\n",
    "last_char_matrix = tf.slice(logits, [0,n_steps-1,0], [1,1,vocab_size])\n",
    "last_char_vector = tf.reshape(last_char_matrix, [vocab_size])\n",
    "probs = tf.nn.softmax(last_char_vector)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#\n",
    "# graph execution\n",
    "#\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 50     # sequences per batch\n",
    "n_batches = 50     # batches per epoch\n",
    "sample_size = 200   # length of generated text at end of each epoch\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size, X_dat, y_dat):\n",
    "    m = X_dat.shape[0]\n",
    "    np.random.seed(epoch * n_batches + batch_index)\n",
    "    indices = np.random.randint(m, size=batch_size)\n",
    "    X_batch = X_dat[indices] \n",
    "    y_batch = y_dat[indices]\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, iteration, batch_size, X_modified, y_modified)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        print(epoch, \"loss:\", loss_val)\n",
    "        \n",
    "        # every so often, generate some sample output\n",
    "        ixes = X_batch[[0]].tolist()[0]\n",
    "        for i in range(sample_size):\n",
    "            sample_batch = np.array(ixes[-n_steps:]).reshape(1, n_steps)\n",
    "            p = probs.eval(feed_dict={X: sample_batch})\n",
    "            ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "            ixes.append(ix)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in ixes[-sample_size:])\n",
    "        print('----\\n %s \\n----' % (txt, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is a copy of the base code above.  Your job is to turn the character-level model into a word-level.  This means that instead of training on sequences of 25 characters, it will train on sequences of 25 words.  Don't worry, this is not difficult.  The structure of the model can stay the same, but now you will have a vocabulary size of thousands of words, instead of a vocabulary size of about 70 characters.\n",
    "\n",
    "You should make the smallest changes you need to make to change the base code into a word-level model.\n",
    "\n",
    "Hint 1: to read a file into an array of words, you may want to look at these links:\n",
    "- https://stackoverflow.com/questions/16922214/reading-a-text-file-and-splitting-it-into-single-words-in-python\n",
    "- https://stackoverflow.com/questions/367155/splitting-a-string-into-words-and-punctuation\n",
    "\n",
    "Hint 2: word-level models use more space; you'll probably need to reduce some of the parameters of the model to get it to run.  Don't worry if your machine still runs out of memory after you greatly reduce the parameters; I will check to see if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training sequences, and corresponding labels\n",
    "X = []\n",
    "y = []\n",
    "seq_length = 50\n",
    "for i in range(0, len(data)-seq_length-1, 1):\n",
    "    X.append([char_to_ix[ch] for ch in data[i:i+seq_length]])\n",
    "    y.append([char_to_ix[ch] for ch in data[i+1:i+seq_length+1]])\n",
    "\n",
    "# reshape the data; in X_modified, each row is an encoded \n",
    "# sequence of characters\n",
    "X_modified = np.reshape(X, (len(X), seq_length))\n",
    "y_modified = np.reshape(y, (len(y), seq_length))\n",
    "\n",
    "#\n",
    "# graph construction\n",
    "#\n",
    "\n",
    "n_steps = seq_length\n",
    "n_inputs = vocab_size\n",
    "n_outputs = vocab_size\n",
    "n_neurons = 100\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, n_steps])\n",
    "X_hot = tf.one_hot(X, n_inputs)\n",
    "y = tf.placeholder(tf.int32, [None, n_steps])\n",
    "\n",
    "# alternative: GPU-enabled GRU cell\n",
    "#        tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell(num_units=n_neurons),\n",
    "\n",
    "basic_cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "              tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons),\n",
    "              output_size=n_outputs)\n",
    "logits, states = tf.nn.dynamic_rnn(basic_cell, X_hot, dtype=tf.float32)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "# Get the last character of the first batch, then convert it\n",
    "# to a probability vector.\n",
    "last_char_matrix = tf.slice(logits, [0,n_steps-1,0], [1,1,vocab_size])\n",
    "last_char_vector = tf.reshape(last_char_matrix, [vocab_size])\n",
    "probs = tf.nn.softmax(last_char_vector)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#\n",
    "# graph execution\n",
    "#\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 50     # sequences per batch\n",
    "n_batches = 50     # batches per epoch\n",
    "sample_size = 200   # length of generated text at end of each epoch\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size, X_dat, y_dat):\n",
    "    m = X_dat.shape[0]\n",
    "    np.random.seed(epoch * n_batches + batch_index)\n",
    "    indices = np.random.randint(m, size=batch_size)\n",
    "    X_batch = X_dat[indices] \n",
    "    y_batch = y_dat[indices]\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, iteration, batch_size, X_modified, y_modified)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        print(epoch, \"loss:\", loss_val)\n",
    "        \n",
    "        # every so often, generate some sample output\n",
    "        ixes = X_batch[[0]].tolist()[0]\n",
    "        for i in range(sample_size):\n",
    "            sample_batch = np.array(ixes[-n_steps:]).reshape(1, n_steps)\n",
    "            p = probs.eval(feed_dict={X: sample_batch})\n",
    "            ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "            ixes.append(ix)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in ixes[-sample_size:])\n",
    "        print('----\\n %s \\n----' % (txt, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is a copy of the base code above.  Your job is to modify the code so that it uses 2 layers of cells rather than just one.  You should modify the base code as little as possible to make this change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training sequences, and corresponding labels\n",
    "X = []\n",
    "y = []\n",
    "seq_length = 50\n",
    "for i in range(0, len(data)-seq_length-1, 1):\n",
    "    X.append([char_to_ix[ch] for ch in data[i:i+seq_length]])\n",
    "    y.append([char_to_ix[ch] for ch in data[i+1:i+seq_length+1]])\n",
    "\n",
    "# reshape the data; in X_modified, each row is an encoded \n",
    "# sequence of characters\n",
    "X_modified = np.reshape(X, (len(X), seq_length))\n",
    "y_modified = np.reshape(y, (len(y), seq_length))\n",
    "\n",
    "#\n",
    "# graph construction\n",
    "#\n",
    "\n",
    "n_steps = seq_length\n",
    "n_inputs = vocab_size\n",
    "n_outputs = vocab_size\n",
    "n_neurons = 100\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, n_steps])\n",
    "X_hot = tf.one_hot(X, n_inputs)\n",
    "y = tf.placeholder(tf.int32, [None, n_steps])\n",
    "\n",
    "# alternative: GPU-enabled GRU cell\n",
    "#        tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell(num_units=n_neurons),\n",
    "\n",
    "basic_cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "              tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons),\n",
    "              output_size=n_outputs)\n",
    "logits, states = tf.nn.dynamic_rnn(basic_cell, X_hot, dtype=tf.float32)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "# Get the last character of the first batch, then convert it\n",
    "# to a probability vector.\n",
    "last_char_matrix = tf.slice(logits, [0,n_steps-1,0], [1,1,vocab_size])\n",
    "last_char_vector = tf.reshape(last_char_matrix, [vocab_size])\n",
    "probs = tf.nn.softmax(last_char_vector)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#\n",
    "# graph execution\n",
    "#\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 50     # sequences per batch\n",
    "n_batches = 50     # batches per epoch\n",
    "sample_size = 200   # length of generated text at end of each epoch\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size, X_dat, y_dat):\n",
    "    m = X_dat.shape[0]\n",
    "    np.random.seed(epoch * n_batches + batch_index)\n",
    "    indices = np.random.randint(m, size=batch_size)\n",
    "    X_batch = X_dat[indices] \n",
    "    y_batch = y_dat[indices]\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, iteration, batch_size, X_modified, y_modified)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        print(epoch, \"loss:\", loss_val)\n",
    "        \n",
    "        # every so often, generate some sample output\n",
    "        ixes = X_batch[[0]].tolist()[0]\n",
    "        for i in range(sample_size):\n",
    "            sample_batch = np.array(ixes[-n_steps:]).reshape(1, n_steps)\n",
    "            p = probs.eval(feed_dict={X: sample_batch})\n",
    "            ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "            ixes.append(ix)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in ixes[-sample_size:])\n",
    "        print('----\\n %s \\n----' % (txt, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
