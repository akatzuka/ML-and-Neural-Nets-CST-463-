{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aj8nRkS0am3K"
   },
   "source": [
    "# Using CNN on Image dataset (CIFAR-10)\n",
    "\n",
    "#### Authors: Cameron Morefield, Sean Vucinich, Bret Stine\n",
    "#### Date: 11.9.2018\n",
    "\n",
    "## Introduction:\n",
    "\n",
    "This CNN (Convolutional Neural Network) looks to modify the pre-existing code for the MNIST dataset in order to attempt to classify CIFAR Images that are color (3 Channel) and 32 pixels in height width.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j0C2w4rWbJiZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JhA-FysEcKh3"
   },
   "source": [
    "Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nmgvTI8abMms"
   },
   "outputs": [],
   "source": [
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "n_inputs = height * width * channels\n",
    "\n",
    "conv1_fmaps = 32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "conv2_fmaps = 64\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "pool3_fmaps = conv2_fmaps\n",
    "\n",
    "n_fc1 = 64\n",
    "n_outputs = 10\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "orpcDgI-bbcU"
   },
   "source": [
    "Value Initializations of our 32x32 images including the adding of RGB color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xYAbmkaBXifh"
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "    X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")\n",
    "\n",
    "with tf.name_scope(\"pool3\"):\n",
    "    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 8 * 8])\n",
    "\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.layers.dense(pool3_flat, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-c82Wmrbrv6"
   },
   "source": [
    "Creation and architecture of our neural net. We changed the pool3 to 8 to be a valid subset of the 32x32 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HTvISYxqbMnh"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "cifar_file = \"D:\\CST 463\\cifar-10-python.tar\\cifar-10-batches-py\\data_batch_1\"\n",
    "\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "cf = unpickle(cifar_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R5wjajEYcKT9"
   },
   "source": [
    "Our functions for creating batches, as well as converting the CIFAR data to usable python dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IxgL8AahbAkL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\envs\\tensorflow_gpuenv\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "D:\\Programs\\Anaconda\\envs\\tensorflow_gpuenv\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# get the data and labels\n",
    "dat = cf[b'data']\n",
    "labels = np.array(cf[b'labels'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "dat_scaled = scaler.fit_transform(dat)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat_scaled, labels, test_size=0.25)\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OWeqDenccXdM"
   },
   "source": [
    "Creating our test and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y1a2k8Tha1aV"
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7oUpTVe7cpL1"
   },
   "source": [
    "Execution phase of the CNN - Running on our data.\n",
    "\n",
    "\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Our CNN ended up getting around a 40% This could be improved by modifying various hyper parameters as well as using a using different batches. We did not look into the contents of each of the binary CIFAR files, so we do not know whether there is any form of randomization in the images, which would have a real effect on our accuracy rate. In addition, if the image data is scaled before the execution of the CNN, then the validation accuracy of the neural net improves by roughly 12%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjLahDpKf7MD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Homework 10",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
