{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CST463_finalProject.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "vIOZqgsF6IZF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Predicting words using a modified Karpathy RNN\n",
        "##### Authors: Bret Stine, Sean Vucinich\n",
        "##### Date: December 14th, 2018\n",
        "\n",
        "## Introduction:\n",
        "For this project, we work off Andrej Karpathy's RNN and modify it to work with tensorflow. Karpathy's code takes a manual approach to predicting words via an RNN. Translating his work into tensorflow allows for streamlined and easy to understand process, assuming the reader is familiar with tensorflow machine learning. \n",
        "\n",
        "For the modified Karpathy RNN, we will be using the Alice in Wonderland text file that was provided to us. The goal for this project is to produce text that will learn and improve on itself to output coherent sentences, or at the very least legible words."
      ]
    },
    {
      "metadata": {
        "id": "W-1Yvw9Ah5BP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Challenges and Issues:**"
      ]
    },
    {
      "metadata": {
        "id": "JAF10Di7iVEL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Throughout the creation of this notebook, numerous issues were encountered that needed to be solved in order to have a working RNN based upon the Karpathy code.\n",
        "\n",
        "* **Understanding Karpathy's code**: The first challenge was getting a basic understanding of Karpathy's code, as it isnâ€™t the easiest thing to look at, the variables aren't descriptive without reading through the code many times, amongst other problems. This was overcome through breaking down the code into sections and running line-by-line to understand what everything accomplishes.\n",
        "\n",
        "*  **Generating a Basic RNN**: The second issue, once the Karpathy code was understood, was how to take it and build an RNN to continue to build upon. This issue was overcome by starting with the basic RNN code from a previous lab and adapting it to work as the framework for the code in this notebook.\n",
        "\n",
        "*  **Using the sparse softmax function**: When we were working on the graph construction of the RNN, one of the first problems we ran into was the choosing the loss function, as the framework RNN we had decided to work upon used the **tf.nn.sparse.softmax_cross_entropy_with_logits()** function. This function however was incompatible with our RNN output due to a mismatch of the rank of the tensors. Through exploring other loss functions, we were able to avoid this issue by switching to the **tf.nn.softmax_cross_entropy_with_logits_v2()** which not only performed better, but bypassed the original issue.\n",
        "\n",
        "* **GPU enabled RNN Cell (tf.contrib.cudnn_rnn.CudnnRNNRelu)**: This issue had to do with switching the basic cell used for the RNN graph with a GPU enabled one, in an effort to vastly improve training performance. However, we had to abandon this idea early on, as the Relu variation of the Cudnn enabled RNN cell outputs differently than the **tf.contrib.rnn.BasicRNNCell()** outputs. This would have necessitated a complete re-write of the graph construction part of the graph, which while extremely beneficial to training speed, would have taken away from completing the project.\n",
        "\n",
        "* **One Hot Encoding - How to do it**: We ran into two issues when it came to one-hot encoding of the batches, the first was exactly should we preform the one hot encoding. From looking through the Karparthy code, and examples of one-hot encoding of text in TensorFlow, we came up with two options. We could either try to use the **tf.one_hot()** function, or we could write our own, and after playing around with the built-in function in TensorFlow with little success, we decided to write our own simple function. To do this we simply take the input then preform a **np.eye()** function upon the input array, which returns a 2-D array with ones on the diagonal and zeros elsewhere.\n",
        "\n",
        "* **One Hot Encoding - Location in the code**: The second issue when it came to preform one-hot encoding was where should we actually preform the function. Initially we thought we should one-hot the text when we reshape it as an encoded sequence of characters, as part of the last pre-processing step. However, this caused problems during the graph execution phase, which resulted in us taking a step back and thinking again about where the text should be encoded. We ultimately decided that we will one-hot encode the batches when they are called in our **next_batch()** function during the graph execution phase, as this resulted in no performance loss and avoided the previous errors.\n",
        "\n",
        "* **Rank of inputs and targets**: This was an unexpected issue that arose early on, and that was having rank mismatching on the input and target tensors. Due to us basing our code upon an earlier RNN graph example, we had initially believed that the tensor only needed the single **seq_length** parameter for it to work properly. While the input tensor did continue without error, the target tensor did not, which prompted us to go back and figure out that *both* tensors needed to have a rank of 3 for our RNN to function properly.\n",
        "\n",
        "* **Next Batch Function**: During the graph execution phase, we had initially wanted to use the **shuffle_batch()** function that we had previously used on other neural net problems, but using that function led to errors and we had to quickly rule it out. The solution to this issue came from inspiration in the textbook *Hands-On Machine Learning with Scikit-Learn & TensorFlow*, which has in the RNN chapter an example **next_batch()** function. Using this function as a base, we modified it a bit, and incorporated our one-hot encoding into the return, which solved the issues we had encountered with the shuffle batch.\n",
        "\n",
        "* **Generating Predictions**: This issue popped up towards the end of the project, once we had the RNN functioning, we had to generate predictions from it. We had quickly realized that we had not done this using an RNN before, so we once again fell back upon the textbook for assistance. While the textbook only had a basic example of how to generate predications, combined with the knowledge of generating predications from previous output on other assignments, and the general format of what that would look like, we were able to write a loop to generate them using the output tensor as such: ***predictions = outputs.eval(feed_dict={inputs: X_batch})***\n",
        "\n",
        "* **Outputting text**: Once the previous issue was solved, we thought that outputting the text from the predictions would be easy, however it turned out to be time consuming. Initially we had thought we needed to simply assign the predictions to letters in a loop, however we quickly realized that the predictions were float values so of course, we could not follow that approach. We then fell back to the Karpathy code for assistance, where upon dissecting the **sample()** function, we learned the general format of how we could sample indices from our predictions by generating probabilities, and then use that to output text, by randomly choosing a character using the probabilities as weights.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yp6yfHKwwnVm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Importing Data:"
      ]
    },
    {
      "metadata": {
        "id": "a_SVx2kW5_LU",
        "colab_type": "code",
        "outputId": "1928779b-a5fb-47ae-b0af-345284c70fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import requests\n",
        "\n",
        "data = requests.get('https://raw.githubusercontent.com/bretstine/alice/master/alice.txt')\n",
        "data = data.text\n",
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print ('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data has 147726 characters, 70 unique.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eTHqefgWbALy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The only libraries needed are numpy and tensorflow. The above code is provided by Karpathy. This takes in a file, gets the unique characters used, and maps letters to numbers. To better utilize the map of letters to numbers, a training set with labels is required to use machine learning."
      ]
    },
    {
      "metadata": {
        "id": "KMM4ABL_cSJ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "hidden_size = 100 # size of hidden layer of neurons\n",
        "seq_length = 25 # number of steps to unroll the RNN for\n",
        "learning_rate = 0.0065\n",
        "\n",
        "# create training sequences and corresponding labels\n",
        "Xi = []\n",
        "yi = []\n",
        "for i in range(0, len(data)-seq_length-1, 1):\n",
        "        Xi.append([char_to_ix[ch] for ch in data[i:i+seq_length]])\n",
        "        yi.append([char_to_ix[ch] for ch in data[i+1:i+seq_length+1]])\n",
        "# reshape the data\n",
        "        \n",
        "# in X_modified, each row is an encoded sequence of characters\n",
        "X_modified = np.reshape(Xi, (len(Xi), seq_length))\n",
        "y_modified = np.reshape(yi, (len(yi), seq_length))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GDT7NpKycbdm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Above is code provided by Dr. Bruns to create training sets with corresponding labels. \n",
        "\n",
        "## Graph Construction Phase:"
      ]
    },
    {
      "metadata": {
        "id": "BUiN8OUzd3Gq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "656d376e-a5a7-4d22-d76b-71dbd5bd3b73"
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "inputs = tf.placeholder(tf.float32, [None, seq_length, vocab_size], name='inputs')\n",
        "targets = tf.placeholder(tf.float32, [None, seq_length, vocab_size], name='targets')\n",
        "\n",
        "with tf.name_scope(\"rnn\"):\n",
        "    basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=vocab_size, activation=tf.nn.relu)\n",
        "    outputs, states = tf.nn.dynamic_rnn(basic_cell, inputs, dtype=tf.float32)\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=outputs, labels=tf.cast(targets, tf.int32))\n",
        "    loss = tf.reduce_mean(xentropy)\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-805826463def>:7: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "17gFDbAueOgi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For the graph construction phase, we used the standard BasicRNNCell with dynamic_rnn as they were the best performing. To account for loss, softmax_cross_entropy_with_logits_v2 was used. We ran into an issue here because sparse_softmax_cross_entropy_with_logits was originally used. However, due to the mismatch of ranks due to the labels, we went first mentioned loss function. Training has remained the same as the last few labs and homeworks with using AdamOptimizer to minimize loss."
      ]
    },
    {
      "metadata": {
        "id": "S_MFLLvAaa3p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot(X, y):\n",
        "    return (np.eye(vocab_size)[X], np.eye(vocab_size)[y])\n",
        "\n",
        "def next_batch(batch_size, inputs, targets):\n",
        "    rnd_idx = np.arange(0 , len(inputs))\n",
        "    np.random.shuffle(rnd_idx)\n",
        "    rnd_idx = rnd_idx[:batch_size]\n",
        "    inputs_shuffle = [inputs[i] for i in rnd_idx]\n",
        "    targets_shuffle = [targets[i] for i in rnd_idx]\n",
        "    return one_hot(np.asarray(inputs_shuffle), np.asarray(targets_shuffle))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G15KrWE7xhZQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To account for one-hot-encoding, we created a custom function utilizing the np.eye function that created an array the same size of the inputted matrix. This function ties into the next_batch function that operates like that of the shuffle_batch from the book."
      ]
    },
    {
      "metadata": {
        "id": "DsdtVhYTdPzM",
        "colab_type": "code",
        "outputId": "788f2224-90b8-47f2-9fcb-9ffa644cc086",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1200
        }
      },
      "cell_type": "code",
      "source": [
        "n_iterations = 10001\n",
        "batch_size = vocab_size\n",
        "smooth_loss = -np.log(1.0/vocab_size) * seq_length\n",
        "\n",
        "with tf.Session() as sess:\n",
        "     init.run()\n",
        "     for iteration in range(n_iterations):\n",
        "         X_batch, y_batch = next_batch(batch_size, X_modified, y_modified)\n",
        "         sess.run(training_op, feed_dict={inputs: X_batch, targets: y_batch})\n",
        "         if iteration % 100 == 0:\n",
        "             test = []\n",
        "             predictions = outputs.eval(feed_dict={inputs: X_batch})\n",
        "             for j in predictions:\n",
        "                 p = np.exp(j) / np.sum(np.exp(j))\n",
        "                 ix = np.random.choice(range(vocab_size), p=np.sum(p,axis=0))\n",
        "                 x = np.zeros((vocab_size, 1))\n",
        "                 x[ix] = 1\n",
        "                 test.append(ix)\n",
        "             txt = ''.join(ix_to_char[ix] for ix in test)\n",
        "             if iteration % 2000 == 0:\n",
        "                 print ('----\\n%s \\n----' % (txt, ))\n",
        "                 mse = loss.eval(feed_dict={inputs: X_batch, targets: y_batch})\n",
        "                 smooth_loss = smooth_loss * 0.999 + mse * 0.001\n",
        "                 print ('iter %d \\tloss: %f \\tsmooth_loss: %f' % (iteration, mse, smooth_loss))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----\n",
            "\n",
            "VyNhJh(THJLFMI UhX.W\n",
            "TFusyb\ryLEflh()-:;tMajKpN!HLFPCtPcEmYvYzxYwYhfPg \n",
            "----\n",
            "iter 0 \tloss: 4.215692 \tsmooth_loss: 106.110384\n",
            "----\n",
            "X enoottYehoylf elea \n",
            "hlk\n",
            "onrrirhea nhyde\n",
            "r , h\n",
            "J\n",
            "ah h \n",
            "----\n",
            "iter 2000 \tloss: 2.199935 \tsmooth_loss: 106.006474\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-c8fc6879ec14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m      \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m          \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_modified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_modified\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m          \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m          \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m              \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8bjK--_Vj7Fx",
        "colab_type": "code",
        "outputId": "ddfb58cb-c797-4bd6-a0a5-39a91ecf9eac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "n_iterations = 20001\n",
        "batch_size = vocab_size\n",
        "smooth_loss = -np.log(1.0/vocab_size) * seq_length\n",
        "\n",
        "with tf.Session() as sess:\n",
        "     init.run()\n",
        "     for iteration in range(n_iterations):\n",
        "         X_batch, y_batch = next_batch(batch_size, X_modified, y_modified)\n",
        "         sess.run(training_op, feed_dict={inputs: X_batch, targets: y_batch})\n",
        "         if iteration % 100 == 0:\n",
        "             test = []\n",
        "             predictions = outputs.eval(feed_dict={inputs: X_batch})\n",
        "             for j in predictions:\n",
        "                 p = np.exp(j) / np.sum(np.exp(j))\n",
        "                 ix = np.random.choice(range(vocab_size), p=np.sum(p,axis=0))\n",
        "                 x = np.zeros((vocab_size, 1))\n",
        "                 x[ix] = 1\n",
        "                 test.append(ix)\n",
        "             txt = ''.join(ix_to_char[ix] for ix in test)\n",
        "             mse = loss.eval(feed_dict={inputs: X_batch, targets: y_batch})\n",
        "             smooth_loss = smooth_loss * 0.999 + mse * 0.001\n",
        "             if iteration % 20000 == 0:\n",
        "                 print ('----\\n%s \\n----' % (txt, ))\n",
        "                 print ('iter %d \\tloss: %f \\tsmooth_loss: %f' % (iteration, mse, smooth_loss))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----\n",
            "MeEMYzMwgth w\"MRECrXw;rdjBEuJ(A.nz;p,;!\r\":CT![DLlHH\"eCZLJ I:grWut].Upr \n",
            "----\n",
            "iter 0 \tloss: 4.218764 \tsmooth_loss: 106.110387\n",
            "----\n",
            "d\n",
            "'e\n",
            "soeuiddodouriucs\n",
            " \n",
            "\n",
            "luee\n",
            " iead\n",
            "eintgrs aeeo \n",
            "----\n",
            "iter 20000 \tloss: 1.828715 \tsmooth_loss: 87.217526\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rvg0xWJBiHq2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Conclusion:**"
      ]
    },
    {
      "metadata": {
        "id": "RlgJHxBIiFvh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This was by far the best project we have worked on. To our knowledge, this is the first time we are producing a visual result besides simply accuracy. Though our output was not the greatest, this means that there is much room for improvement and above all, learning. We will continue to work on this to produce better results as a potential resume builder. Having a concentration in data science, we know that this is only the beginning. We are motivated to always do better, and this project has been an experience all on its own. A few important things we learned, going through a manual version make us think about how the program runs and how complicated machine learning can be. From this, we will strive to do better and continue learning. When we finally outputted text that improved upon itself, we celebrated and were incredibly excited. We do not think any project besides this one would have been as satisfying."
      ]
    }
  ]
}